
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Seven Story Rabbit Hole</title>
  <meta name="author" content="Traun Leyden">

  
  <meta name="description" content="I found the official docs on using Google Protocol Buffers from Go a bit confusing, and couldn&rsquo;t find any other clearly written blog posts on &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://tleyden.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Seven Story Rabbit Hole" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>

  <table><tr><td>
  <h1><a href="/">Seven Story Rabbit Hole</a></h1>
  
    <h2>Sometimes awesome things happen in deep rabbit holes.  Or not.</h2>
  
  </td><td>&nbsp;&nbsp;
  <img class="left" src="/images/rabbit_hole_graphic.png" width="132" height="105" title="image" alt="images">
  </td></tr>
  </table>

</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:tleyden.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/02/getting-started-with-go-and-protocol-buffers/">Getting Started With Go and Protocol Buffers</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-12-02T06:32:00-08:00" pubdate data-updated="true">Dec 2<span>nd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I found the <a href="https://github.com/golang/protobuf">official docs</a> on using Google Protocol Buffers from Go a bit confusing, and couldn&rsquo;t find any other clearly written blog posts on the subject, so I figured I&rsquo;d write my own.</p>

<p>This will walk you through the following:</p>

<ul>
<li>Install golang/protobuf and required dependencies</li>
<li>Generating Go wrappers for a test protocol buffer definition</li>
<li>Using those Go wrappers to marshal and unmarshal an object</li>
</ul>


<h2>Install protoc binary</h2>

<p>Since the protocol buffer compiler <code>protoc</code> is required later, we must install it.</p>

<p><strong>Ubuntu 14.04</strong></p>

<p>If you want to use an older version (v2.5), simply do:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get install protobuf-compiler</span></code></pre></td></tr></table></div></figure>


<p>Otherwise if you want the latest version (v2.6):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get install build-essential
</span><span class='line'>$ wget https://protobuf.googlecode.com/svn/rc/protobuf-2.6.0.tar.gz
</span><span class='line'>$ tar xvfz protobuf-2.6.0.tar.gz
</span><span class='line'>$ cd protobuf-2.6.0
</span><span class='line'>$ ./configure && make install</span></code></pre></td></tr></table></div></figure>


<p><strong>OSX</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ brew install protobuf</span></code></pre></td></tr></table></div></figure>


<h2>Install Go Protobuf library</h2>

<p>This assumes you have Go 1.2+ or later already installed, and your <code>$GOPATH</code> variable set.</p>

<p>In order to generate Go wrappers, we need to install the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ go get -u -v github.com/golang/protobuf/proto
</span><span class='line'>$ go get -u -v github.com/golang/protobuf/protoc-gen-go</span></code></pre></td></tr></table></div></figure>


<h2>Download a test .proto file</h2>

<p>In order to generate wrappers, we need a <code>.proto</code> file with object definitions.</p>

<p>This one is a slightly modified version of the one from the <a href="https://github.com/golang/protobuf">official docs</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget https://gist.githubusercontent.com/tleyden/95de4bfe34321c79e91b/raw/f8696fe0f1462f377d6bd13c5f20cccfa182578a/test.proto</span></code></pre></td></tr></table></div></figure>


<h2>Generate Go wrappers</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ protoc --go_out=. *.proto</span></code></pre></td></tr></table></div></figure>


<p>You should end up with a new file generated: <code>test.pb.go</code></p>

<h2>Marshalling and unmarshalling an object</h2>

<p>Open a new file <code>main.go</code> in <a href="http://tleyden.github.io/blog/2014/05/22/configure-emacs-as-a-go-editor-from-scratch/">emacs</a> or your favorite editor, and paste the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>package main
</span><span class='line'>
</span><span class='line'>import (
</span><span class='line'>  "log"
</span><span class='line'>
</span><span class='line'>  "github.com/golang/protobuf/proto"
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>func main() {
</span><span class='line'>
</span><span class='line'>  test := &Test{
</span><span class='line'>      Label: proto.String("hello"),
</span><span class='line'>      Type:  proto.Int32(17),
</span><span class='line'>      Optionalgroup: &Test_OptionalGroup{
</span><span class='line'>          RequiredField: proto.String("good bye"),
</span><span class='line'>      },
</span><span class='line'>  }
</span><span class='line'>  data, err := proto.Marshal(test)
</span><span class='line'>  if err != nil {
</span><span class='line'>      log.Fatal("marshaling error: ", err)
</span><span class='line'>  }
</span><span class='line'>  newTest := &Test{}
</span><span class='line'>  err = proto.Unmarshal(data, newTest)
</span><span class='line'>  if err != nil {
</span><span class='line'>      log.Fatal("unmarshaling error: ", err)
</span><span class='line'>  }
</span><span class='line'>  // Now test and newTest contain the same data.
</span><span class='line'>  if test.GetLabel() != newTest.GetLabel() {
</span><span class='line'>      log.Fatalf("data mismatch %q != %q", test.GetLabel(), newTest.GetLabel())
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  log.Printf("Unmarshalled to: %+v", newTest)
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Explanation:</p>

<ul>
<li>Lines 11-14: Create a new object suitable for protobuf marshalling and populate it&rsquo;s fields.  <em>Note that using <code>proto.String(..)</code> / <code>proto.Int32(..)</code> isn&rsquo;t strictly required, they are just convencience wrappers to get string / int pointers</em>.</li>
<li>Line 18: Marshal to a byte array.</li>
<li>Line 22: Create a new empty object.</li>
<li>Line 23: Unmarshal previously marshalled byte array into new object</li>
<li>Line 28: Verify that the &ldquo;label&rdquo; field made the marshal/unmarshall round trip safely</li>
</ul>


<p><strong>Run it via:</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ go run main.go test.pb.go</span></code></pre></td></tr></table></div></figure>


<p>and you should see the output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Unmarshalled to: label:"hello" type:17 OptionalGroup{RequiredField:"good bye" }  </span></code></pre></td></tr></table></div></figure>


<p>Congratulations!  You are now using protocol buffers from Go.</p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/golang/protobuf">Official golang/protobuf repo</a></li>
<li><a href="https://code.google.com/p/gogoprotobuf/">gogoprotobuf fork</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/14/running-cbfs/">Running a CBFS Cluster on CoreOS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-14T06:43:00-08:00" pubdate data-updated="true">Nov 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This will walk you through getting a cbfs cluster up and running.</p>

<h2>What is CBFS?</h2>

<p>cbfs is a distributed filesystem on top of Couchbase Server, not unlike Mongo&rsquo;s GridFS or Riak&rsquo;s CS.</p>

<p>Here&rsquo;s a typical deployment architecture:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs-overview.png" alt="cbfs overview" /></p>

<p>Although not shown, all cbfs daemons can communicate with all Couchbase Server instances.</p>

<p>It is not required to run cbfs on the same machine as Couchbase Server, but it <em>is</em> meant to be run in the same data center as Couchbase Server.</p>

<p>If you want a deeper understanding of how cbfs works, check the <a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a> or this <a href="http://dustin.sallings.org/2012/09/27/cbfs.html">blog post</a>.</p>

<h2>Kick off a Couchbase Cluster</h2>

<p>cbfs depends on having a Couchbase cluster running.</p>

<p>Follow all of the steps in <a href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/">Running Couchbase Cluster Under CoreOS on AWS</a> to kick off a 3 node Couchbase cluster.</p>

<h2>Add security groups</h2>

<p>A few ports will need to be opened up for cbfs.</p>

<p>Go to the AWS console andeEdit the Couchbase-CoreOS-CoreOSSecurityGroup-xxxx security group and add the following rules:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Type             Protocol  Port Range Source  
</span><span class='line'>----             --------  ---------- ------
</span><span class='line'>Custom TCP Rule  TCP       8484       Custom IP: sg-6e5a0d04 (copy and paste from port 4001 rule)
</span><span class='line'>Custom TCP Rule  TCP       8423       Custom IP: sg-6e5a0d04 </span></code></pre></td></tr></table></div></figure>


<p>At this point your security group should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/security_group_cbfs.png" alt="security group" /></p>

<h1>Create a new bucket for cbfs</h1>

<p><strong>Open Couchbase Server Admin UI</strong></p>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>In your browser, go to <code>http://&lt;public_ip&gt;:8091/</code></p>

<p><strong>Create Bucket</strong></p>

<p>Go to Data Buckets / Create New Bucket</p>

<p>Enter <strong>cbfs</strong> for the name of the bucket.</p>

<p>Leave all other settings as default.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs_create_bucket.png" alt="create bucket" /></p>

<h2>ssh in</h2>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>ssh into one of the machines:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh -A core@&lt;public_ip&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Run cbfs</h2>

<p><strong>Create a volume dir</strong></p>

<p>Since the fileystem of a docker container is not meant for high throughput io, a volume should be used for cbfs.</p>

<p>Create a directory on the host OS (i.e., on the Core OS instance)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo mkdir -p /var/lib/cbfs/data
</span><span class='line'>$ sudo chown -R core:core /var/lib/cbfs</span></code></pre></td></tr></table></div></figure>


<p>This will be mounted by the docker container in the next step.</p>

<p><strong>Generate fleet unit files</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget https://gist.githubusercontent.com/tleyden/d70161c3827cb8b788a8/raw/8f6c81f0095b0007565e9b205e90afb132552060/cbfs_node.service.template
</span><span class='line'>$ for i in `seq 1 3`; do cp cbfs_node.service.template cbfs_node.$i.service; done</span></code></pre></td></tr></table></div></figure>


<p><strong>Start cbfs on all cluster nodes</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fleetctl start cbfs_node.*.service</span></code></pre></td></tr></table></div></figure>


<p>Run <code>fleetctl list-units</code> to list the  units running in your cluster.  You should have the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fleetctl list-units
</span><span class='line'>UNIT                                            MACHINE                         ACTIVE    SUB
</span><span class='line'>cbfs_node.1.service                             6ecff20c.../10.51.177.81        active    running
</span><span class='line'>cbfs_node.2.service                             b8eb6653.../10.79.155.153       active    running
</span><span class='line'>cbfs_node.3.service                             02d48afd.../10.186.172.24       active    running
</span><span class='line'>couchbase_bootstrap_node.service                02d48afd.../10.186.172.24       active    running
</span><span class='line'>couchbase_bootstrap_node_announce.service       02d48afd.../10.186.172.24       active    running
</span><span class='line'>couchbase_node.1.service                        6ecff20c.../10.51.177.81        active    running
</span><span class='line'>couchbase_node.2.service                        b8eb6653.../10.79.155.153       active    running</span></code></pre></td></tr></table></div></figure>


<p><strong>View cbfs output</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fleetctl journal cbfs_node.1.service
</span><span class='line'>2014/11/14 23:18:58 Connecting to couchbase bucket cbfs at http://10.51.177.81:8091/
</span><span class='line'>2014/11/14 23:18:58 Error checking view version: MCResponse status=KEY_ENOENT, opcode=GET, opaque=0, msg: Not found
</span><span class='line'>2014/11/14 23:18:58 Installing new version of views (old version=0)
</span><span class='line'>2014/11/14 23:18:58 Listening to web requests on :8484 as server 10.51.177.81
</span><span class='line'>2014/11/14 23:18:58 Error removing 10.51.177.81's task list: MCResponse status=KEY_ENOENT, opcode=DELETE, opaque=0, msg: Not found
</span><span class='line'>2014/11/14 23:19:05 Error updating space used: Expected 1 result, got []</span></code></pre></td></tr></table></div></figure>


<h2>Run cbfs client</h2>

<p>Run a bash shell in a docker container that has <code>cbfsclient</code> pre-installed:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo docker run -ti --net=host tleyden5iwx/cbfs /bin/bash</span></code></pre></td></tr></table></div></figure>


<p><strong>Upload a file</strong></p>

<p>From within the docker container launched in the previous step:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># echo "foo" &gt; foo
</span><span class='line'># ip=$(hostname -i | tr -d ' ')
</span><span class='line'># cbfsclient http://$ip:8484/ upload foo /foo</span></code></pre></td></tr></table></div></figure>


<p>There should be no errors.  If you run <code>fleetctl journal cbfs_node.1.service</code> again on the CoreOS instance, you should see log messages like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014/11/14 21:51:43 Recorded myself as an owner of e242ed3bffccdf271b7fbaf34ed72d089537b42f: result=success</span></code></pre></td></tr></table></div></figure>


<p><strong>List directory</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># cbfsclient http://$ip:8484/ ls /
</span><span class='line'>foo</span></code></pre></td></tr></table></div></figure>


<p>It should list the foo file we uploaded earlier.</p>

<p>Congratulations!  You now have cbfs up and running.</p>

<h2>References</h2>

<ul>
<li><a href="http://dustin.sallings.org/2012/09/27/cbfs.html">cbfs &ndash; a couchbase large object store</a></li>
<li><a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a></li>
<li><a href="http://cbfs-ext.hq.couchbase.com/dist/">cbfs binary downloads</a></li>
<li><a href="http://github.com/couchbaselabs/cbfs">cbfs github repo</a></li>
<li><a href="https://github.com/couchbaselabs/cbfs/issues/132">cbfs question</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/12/an-example-of-using-nsq-from-go/">An Example of Using NSQ From Go</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-12T07:26:00-08:00" pubdate data-updated="true">Nov 12<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="https://github.com/bitly/nsq">NSQ</a> is a message queue, similar to RabbitMQ.  I decided I&rsquo;d give it a whirl.</p>

<h2>Install Nsq</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget https://s3.amazonaws.com/bitly-downloads/nsq/nsq-0.2.31.darwin-amd64.go1.3.1.tar.gz
</span><span class='line'>$ tar xvfz nsq-0.2.31.darwin-amd64.go1.3.1.tar.gz
</span><span class='line'>$ sudo mv nsq-0.2.31.darwin-amd64.go1.3.1/bin/* /usr/local/bin</span></code></pre></td></tr></table></div></figure>


<h2>Launch Nsq</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ nsqlookupd & 
</span><span class='line'>$ nsqd --lookupd-tcp-address=127.0.0.1:4160 &
</span><span class='line'>$ nsqadmin --lookupd-http-address=127.0.0.1:4161 &</span></code></pre></td></tr></table></div></figure>


<h2>Get Go client library</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ go get -u -v github.com/bitly/go-nsq</span></code></pre></td></tr></table></div></figure>


<h2>Create a producer</h2>

<p>Add the following code to main.go:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>package main
</span><span class='line'>
</span><span class='line'>import (
</span><span class='line'>  "log"
</span><span class='line'>  "github.com/bitly/go-nsq"
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>func main() {
</span><span class='line'>  config := nsq.NewConfig()
</span><span class='line'>  w, _ := nsq.NewProducer("127.0.0.1:4150", config)
</span><span class='line'>
</span><span class='line'>  err := w.Publish("write_test", []byte("test"))
</span><span class='line'>  if err != nil {
</span><span class='line'>      log.Panic("Could not connect")
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  w.Stop()
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<p>and then run it with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ go run main.go</span></code></pre></td></tr></table></div></figure>


<p>If you go to your NSQAdmin at <a href="http://localhost:4171/">http://localhost:4171</a>, you should see a single message in the <code>write_test</code> topic.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nsq_admin.png" alt="NSQAdmin" /></p>

<h2>Create a consumer</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>package main
</span><span class='line'>
</span><span class='line'>import (
</span><span class='line'>  "log"
</span><span class='line'>  "sync"
</span><span class='line'>
</span><span class='line'>  "github.com/bitly/go-nsq"
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>func main() {
</span><span class='line'>
</span><span class='line'>  wg := &sync.WaitGroup{}
</span><span class='line'>  wg.Add(1)
</span><span class='line'>
</span><span class='line'>  config := nsq.NewConfig()
</span><span class='line'>  q, _ := nsq.NewConsumer("write_test", "ch", config)
</span><span class='line'>  q.AddHandler(nsq.HandlerFunc(func(message *nsq.Message) error {
</span><span class='line'>      log.Printf("Got a message: %v", message)
</span><span class='line'>      wg.Done()
</span><span class='line'>      return nil
</span><span class='line'>  }))
</span><span class='line'>  err := q.ConnectToNSQD("127.0.0.1:4150")
</span><span class='line'>  if err != nil {
</span><span class='line'>      log.Panic("Could not connect")
</span><span class='line'>  }
</span><span class='line'>  wg.Wait()
</span><span class='line'>
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<p>and then run it with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ go run main.go</span></code></pre></td></tr></table></div></figure>


<p>You should see output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014/11/12 08:37:29 INF    1 [write_test/ch] (127.0.0.1:4150) connecting to nsqd
</span><span class='line'>2014/11/12 08:37:29 Got a message: &{[48 55 54 52 48 57 51 56 50 100 50 56 101 48 48 55] [116 101 115 116] 1415810020571836511 2 0xc208042118 0 0}</span></code></pre></td></tr></table></div></figure>


<p>Congratulations!  You just pushed a message through <strong>NSQ</strong>.</p>

<h2>Enhanced consumer: use NSQLookupd</h2>

<p>The above example hardcoded the ip of <code>nsqd</code> into the consumer code, which is not a best practice.  A better way to go about it is to point the consumer at <code>nsqlookupd</code>, which will transparently connect to the appropriate <code>nsqd</code> that happens to be publishing that topic.</p>

<p>In our example, we only have a single <code>nsqd</code>, so it&rsquo;s an extraneous lookup.  But it&rsquo;s good to get into the right habits early, especially if you are a <em>habitual copy/paster</em>.</p>

<p>The consumer example only needs a one-line change to get this enhancement:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>err := q.ConnectToNSQLookupd("127.0.0.1:4161")</span></code></pre></td></tr></table></div></figure>


<p>Which will connect to the HTTP port of <code>nsqlookupd</code>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/">CoreOS With Nvidia CUDA GPU Drivers</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-04T07:08:00-08:00" pubdate data-updated="true">Nov 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This will walk you through installing the Nvidia GPU kernel module and CUDA drivers on a docker container running inside of CoreOS.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/coreos-nvidia-gpu.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS on an AWS GPU instance</h2>

<ul>
<li><p>Launch a new EC2 instance</p></li>
<li><p>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-7c8b3f14</strong> (CoreOS-stable-410.1.0-hvm)</p></li>
<li><p>Select the GPU instances: <strong>g2.2xlarge</strong></p></li>
<li><p>Increase root EBS store from 8 GB &ndash;> 20 GB to give yourself some breathing room</p></li>
</ul>


<h2>ssh into CoreOS instance</h2>

<p>Find the public ip of the EC2 instance launched above, and ssh into it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh -A core@ec2-54-80-24-46.compute-1.amazonaws.com</span></code></pre></td></tr></table></div></figure>


<h2>Run Ubuntu 12 docker container in privileged mode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo docker run --privileged=true -i -t ubuntu:12.04 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get update
</span><span class='line'>$ apt-get install build-essential wget git</span></code></pre></td></tr></table></div></figure>


<h2>Prepare CoreOS kernel source</h2>

<p><strong>Clone CoreOS kernel repository</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir -p /usr/src/kernels
</span><span class='line'>$ cd /usr/src/kernels
</span><span class='line'>$ git clone https://github.com/coreos/linux.git</span></code></pre></td></tr></table></div></figure>


<p><strong>Get CoreOS kernel version</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ uname -a
</span><span class='line'>Linux ip-10-183-54-167.ec2.internal 3.15.8+ #2 SMP Fri Sep 26 08:37:17 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux</span></code></pre></td></tr></table></div></figure>


<p>The CoreOS kernel version is <strong>3.15.8</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd linux
</span><span class='line'>$ git checkout remotes/origin/coreos/v3.15.8</span></code></pre></td></tr></table></div></figure>


<p><strong>Create kernel configuration file</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ zcat /proc/config.gz &gt; /usr/src/kernels/linux/.config</span></code></pre></td></tr></table></div></figure>


<p><strong>Prepare kernel source for building modules</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /usr/src/kernels/linux
</span><span class='line'>$ make modules_prepare</span></code></pre></td></tr></table></div></figure>


<p>Now you should be ready to install the nvidia driver.</p>

<h2>Install nvidia driver</h2>

<p><strong>Download</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir -p /opt/nvidia
</span><span class='line'>$ cd /opt/nvidia
</span><span class='line'>$ wget http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run</span></code></pre></td></tr></table></div></figure>


<p><strong>Unpack</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ chmod +x cuda_6.5.14_linux_64.run
</span><span class='line'>$ mkdir nvidia_installers
</span><span class='line'>$ ./cuda_6.5.14_linux_64.run -extract=`pwd`/nvidia_installers</span></code></pre></td></tr></table></div></figure>


<p><strong>Install</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd nvidia_installers
</span><span class='line'>$ ./NVIDIA-Linux-x86_64-340.29.run --kernel-source-path=/usr/src/kernels/linux/</span></code></pre></td></tr></table></div></figure>


<p><strong>Installer Questions</strong></p>

<ul>
<li>Install NVidia&rsquo;s 32-bit compatibility libraries? <strong>YES</strong></li>
<li>Would you like to run nvidia-xconfig? <strong>NO</strong></li>
</ul>


<p>If everything worked, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nvidia_driver_installed.png" alt="nvidia drivers installed" /></p>

<h2>Load nvidia kernel module</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ modprobe nvidia</span></code></pre></td></tr></table></div></figure>


<p>No errors should be returned.  Verify it&rsquo;s loaded by running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ lsmod | grep -i nvidia</span></code></pre></td></tr></table></div></figure>


<p>and you should see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nvidia              10533711  0
</span><span class='line'>i2c_core               41189  2 nvidia,i2c_piix4</span></code></pre></td></tr></table></div></figure>


<h2>Install CUDA</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ./cuda-linux64-rel-6.5.14-18749181.run
</span><span class='line'>$ ./cuda-samples-linux-6.5.14-18745345.run</span></code></pre></td></tr></table></div></figure>


<h2>Verify CUDA</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
</span><span class='line'>$ make
</span><span class='line'>$ ./deviceQuery   </span></code></pre></td></tr></table></div></figure>


<p>You should see the following output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
</span><span class='line'>Result = PASS</span></code></pre></td></tr></table></div></figure>


<p>Congratulations!  You now have a docker container running under CoreOS that can access the GPU.</p>

<h1>Appendix A: Using Core OS Alpha</h1>

<p>The instructions above were for an older version of CoreOS.  The following instructions are for Core OS Alpha, and might possibly work on the current version of CoreOS stable (444.5.0).  Only the parts that differ from above steps are listed:</p>

<h2>Launch CoreOS Alpha on an AWS GPU instance</h2>

<ul>
<li>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-66e6680e</strong> (CoreOS-alpha-490.0.0-hvm)</li>
</ul>


<h2>Run Ubuntu 14 docker container in privileged mode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo docker run --privileged=true -i -t ubuntu:14.04 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p>In order to match the version of gcc that was used to build the CoreOS kernel.  (gcc 4.7)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get update
</span><span class='line'>$ apt-get install gcc-4.7 g++-4.7 wget git make dpkg-dev</span></code></pre></td></tr></table></div></figure>


<p><strong>Set gcc 4.7 as default</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ update-alternatives --remove gcc /usr/bin/gcc-4.8
</span><span class='line'>$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.7 60 --slave /usr/bin/g++ g++ /usr/bin/g++-4.7
</span><span class='line'>$ update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40 --slave /usr/bin/g++ g++ /usr/bin/g++-4.8</span></code></pre></td></tr></table></div></figure>


<p><strong>Verify</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ update-alternatives --config gcc</span></code></pre></td></tr></table></div></figure>


<p>It should list gcc 4.7 with an asterisk next to it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>* 0            /usr/bin/gcc-4.7   60        auto mode</span></code></pre></td></tr></table></div></figure>


<h2>Prepare CoreOS kernel source</h2>

<p><strong>Get CoreOS kernel version</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ uname -a
</span><span class='line'>Linux ip-10-11-167-200.ec2.internal 3.17.2+ #2 SMP Tue Nov 4 04:15:48 UTC 2014 x86_64 Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux</span></code></pre></td></tr></table></div></figure>


<p>The CoreOS kernel version is <strong>3.17.2</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd linux
</span><span class='line'>$ git checkout remotes/origin/coreos/v3.17.2</span></code></pre></td></tr></table></div></figure>


<h1>Appendix B: Expose GPU to other docker containers</h1>

<p>If you need <em>other</em> docker containers on this CoreOS instance to be able to access the GPU, you can do the following steps.</p>

<p><em>Note:</em> you need to be using CoreOS-alpha-490.0.0 or later, since this requires Docker 1.3 to work.</p>

<p><strong>Exit docker container</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ exit</span></code></pre></td></tr></table></div></figure>


<p>You should be back to your CoreOS shell.</p>

<p><strong>Add nvidia device nodes</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget https://gist.githubusercontent.com/tleyden/74f593a0beea300de08c/raw/95ed93c5751a989e58153db6f88c35515b7af120/nvidia_devices.sh
</span><span class='line'>$ chmod +x nvida_devices.sh
</span><span class='line'>$ sudo ./nvida_devices.sh</span></code></pre></td></tr></table></div></figure>


<p><strong>Verify device nodes</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -alh /dev | grep -i nvidia
</span><span class='line'>crw-rw-rw-  1 root root  251,   0 Nov  5 16:37 nvidia-uvm
</span><span class='line'>crw-rw-rw-  1 root root  195,   0 Nov  5 16:37 nvidia0
</span><span class='line'>crw-rw-rw-  1 root root  195, 255 Nov  5 16:37 nvidiactl</span></code></pre></td></tr></table></div></figure>


<p><strong>Launch docker containers</strong></p>

<p>When you launch other docker containers on the same CoreOS instance, to allow them to access the GPU device you will need to add the following arguments:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo docker run -ti --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm tleyden5iwx/ubuntu-cuda /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>A complete example is available in <a href="http://tleyden.github.io/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a>.  You can pick up at th <strong>Run GPU enabled docker image</strong> step.</p>

<h2>References</h2>

<ul>
<li><a href="https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4">https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4</a> &ndash; Thanks Сергей!</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/">Running Couchbase Cluster Under CoreOS on AWS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-01T12:16:00-07:00" pubdate data-updated="true">Nov 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Here are instructions on how to fire up a Couchbase Server cluster running under CoreOS on AWS CloudFormation.  You will end up with the following system:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase-coreos-onion.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS instances via AWS Cloud Formation</h2>

<p>Click the &ldquo;Launch Stack&rdquo; button to launch your CoreOS instances via AWS Cloud Formation:</p>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/coreos-stable-pv.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p><em>NOTE: this is hardcoded to use the us-east-1 region, so if you need a different region, you should edit the URL accordingly</em></p>

<p>Use the following parameters in the form:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>:  use whatever you normally use to start EC2 instances.  For this discussion, let&rsquo;s assumed you used <code>aws</code>, which corresponds to a file you have on your laptop called <code>aws.cer</code></li>
</ul>


<h2>ssh into a CoreOS instance</h2>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com</span></code></pre></td></tr></table></div></figure>


<h2>Sanity check</h2>

<p>Let&rsquo;s make sure the CoreOS cluster is healthy first:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fleetctl list-machines</span></code></pre></td></tr></table></div></figure>


<p>This should return a list of machines in the cluster, like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MACHINE          IP              METADATA
</span><span class='line'>03b08680...     10.33.185.16    -
</span><span class='line'>209a8a2e...     10.164.175.9    -
</span><span class='line'>25dd84b7...     10.13.180.194   -</span></code></pre></td></tr></table></div></figure>


<h2>Download cluster-init script</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget https://raw.githubusercontent.com/couchbaselabs/couchbase-server-docker/master/scripts/cluster-init.sh
</span><span class='line'>$ chmod +x cluster-init.sh</span></code></pre></td></tr></table></div></figure>


<p>This script is not much.  I wrapped things up in a script because the instructions were getting long, but all it does is:</p>

<ul>
<li>Downloads a few fleet init files from github.</li>
<li>Generates a few more fleet init files based on a template and the number of nodes you want.</li>
<li>Stashes the username/password argument you give it into <code>etcd</code>.</li>
<li>Tells <code>fleetctl</code> to kick everything off.  Whee!</li>
</ul>


<h2>Launch cluster</h2>

<p>Run the script you downloaded in the previous step:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ./cluster-init.sh -v 3.0.1 -n 3 -u "user:passw0rd"</span></code></pre></td></tr></table></div></figure>


<p>Where:</p>

<ul>
<li><strong>-v</strong> the version of Couchbase Server to use.  Valid values are 3.0.1 or 2.2.0.</li>
<li><strong>-n</strong> the total number of couchbase nodes to start &mdash; should correspond to number of ec2 instances (eg, 3)</li>
<li><strong>-u</strong> the username and password as a single string, delimited by a colon (:)</li>
</ul>


<p>Replace <code>user:passw0rd</code> with a sensible username and password.  It <strong>must</strong> be colon separated, with no spaces.  The password itself must be at least 6 characters.</p>

<p>Once this command completes, your cluster will be in the process of launching.</p>

<h2>Verify</h2>

<p>To check the status of your cluster, run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fleetctl list-units</span></code></pre></td></tr></table></div></figure>


<p>You should see four units, all as active.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>UNIT                     MACHINE             ACTIVE  SUB
</span><span class='line'>couchbase_bootstrap_node.service                375d98b9.../10.63.168.35  active  running
</span><span class='line'>couchbase_bootstrap_node_announce.service       375d98b9.../10.63.168.35  active  running
</span><span class='line'>couchbase_node.1.service                        8cf54d4d.../10.187.61.136 active  running
</span><span class='line'>couchbase_node.2.service                        b8cf0ed6.../10.179.161.76 active  running</span></code></pre></td></tr></table></div></figure>


<h2>Rebalance Couchbase Cluster</h2>

<p><strong>Login to Couchbase Server Web Admin</strong></p>

<ul>
<li>Find the public ip of any of your CoreOS instances via the AWS console</li>
<li>In a browser, go to <code>http://&lt;instance_public_ip&gt;:8091</code></li>
<li>Login with the username/password you provided above</li>
</ul>


<p>After logging in, your Server Nodes tab should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_prerebalance.png" alt="screenshot" /></p>

<p><strong>Kick off initial rebalance</strong></p>

<ul>
<li>Click server nodes</li>
<li>Click &ldquo;Rebalance&rdquo;</li>
</ul>


<p>After the rebalance is complete, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_admin_ui_post_rebalance.png" alt="screenshot" /></p>

<p>Congratulations!  You now have a 3 node Couchbase Server cluster running under CoreOS / Docker.</p>

<h1>References</h1>

<ul>
<li><a href="https://gist.github.com/dustin/6605182">How I built couchbase 2.2 for docker</a> by <a href="https://twitter.com/dlsspy">@dlsspy</a></li>
<li><a href="https://github.com/tleyden/couchbase-server-coreos">https://github.com/tleyden/couchbase-server-coreos</a></li>
<li><a href="https://registry.hub.docker.com/u/ncolomer/couchbase/">https://registry.hub.docker.com/u/ncolomer/couchbase/</a></li>
<li><a href="https://github.com/lifegadget/docker-couchbase">https://github.com/lifegadget/docker-couchbase</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/30/goroutines-vs-threads/">Goroutines vs Threads</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-30T06:28:00-07:00" pubdate data-updated="true">Oct 30<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Here are some of the advantages of Goroutines over threads:</p>

<ul>
<li>You can run more goroutines on a typical system than you can threads.</li>
<li>Goroutines have growable segmented stacks.</li>
<li>Goroutines have a faster startup time than threads.</li>
<li>Goroutines come with built-in primitives to communicate safely between themselves (channels).</li>
<li>Goroutines allow you to avoid having to resort to mutex locking when sharing data structures.</li>
<li>Goroutines are multiplexed onto a small number of OS threads, rather than a 1:1 mapping.</li>
<li>You can write massively concurrent servers withouth having to resort to evented programming.</li>
</ul>


<h2>You can run more of them</h2>

<p>On Java you can run 1000&rsquo;s or tens of 1000&rsquo;s threads.  On Go you can run hundreds of thousands or millions of goroutines.</p>

<p>Java threads map directly to OS threads, and are relatively heavyweight.  Part of the reason they are heavyweight is their rather large fixed stack size.  This caps the number of them you can run in a single VM due to the increasing memory overhead.</p>

<p>Go OTOH has a segmented stack that grows as needed.  They are &ldquo;Green threads&rdquo;, which means the Go runtime does the scheduling, not the OS.  The runtime multiplexes the goroutines onto real OS threads, the number of which is controlled by <code>GOMAXPROCS</code>.  Typically you&rsquo;ll want to set this to the number of cores on your system, to maximize potential parellelism.</p>

<h2>They let you avoid locking hell</h2>

<p>One of the biggest drawback of threaded programming is the complexity and brittleness of many codebases that use threads to achieve high concurrency.  There can be latent deadlocks and race conditions, and it can become near impossible to reason about the code.</p>

<p>Go OTOH gives you primitives that allow you to avoid locking completely.  The mantra is <em>don&rsquo;t communicate by sharing memory, share memory by communicating</em>.  In other words, if two goroutines need to share data, they can do so safely over a channel.  Go handles all of the synchronization for you, and it&rsquo;s much harder to run into things like deadlocks.</p>

<h2>No callback spaghetti, either</h2>

<p>There are other approaches to achieving high concurrency with a small number of threads.  Python Twisted was one of the early ones that got a lot of attention.  Node.js is currently the most prominent evented frameworks out there.</p>

<p>The problem with these evented frameworks is that the code complexity is also high, and difficult to reason about.  Rather than &ldquo;straightline&rdquo; coding, the programmer is forced to chain callbacks, which gets interleaved with error handling.  While refactoring can help tame some of the mental load, it&rsquo;s still an issue.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/25/running-caffe-on-aws-gpu-instance-via-docker/">Running Caffe on AWS GPU Instance via Docker</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-25T20:42:00-07:00" pubdate data-updated="true">Oct 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This is a tutorial to help you get the <a href="http://caffe.berkeleyvision.org/">Caffe deep learning framework</a> up and running on a GPU-powered AWS instance running inside a Docker container.</p>

<h2>Architecture</h2>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/caffe_docker_aws_onion.png" alt="architecture diagram" /></p>

<h2>Setup host</h2>

<p>Before you can start your docker container, you will need to go <strong>deeper down the rabbit hole</strong>.</p>

<p>You&rsquo;ll first need to complete the steps here:</p>

<p><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">Setting up an Ubuntu 14.04 box running on a GPU-enabled AWS instance</a></p>

<p>After you&rsquo;re done, you&rsquo;ll end up with a host OS with the following properties:</p>

<ul>
<li>A GPU enabled AWS instance running Ubuntu 14.04</li>
<li>Nvidia kernel module</li>
<li>Nvidia device drivers</li>
<li>CUDA 6.5 installed and verified</li>
</ul>


<h2>Install Docker</h2>

<p>Once your host OS is setup, you&rsquo;re ready to install docker.  (version 1.3 at the time of this writing)</p>

<p>Setup the key for the docker repo:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9</span></code></pre></td></tr></table></div></figure>


<p>Add the docker repo:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo sh -c "echo deb https://get.docker.com/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list"
</span><span class='line'>$ sudo apt-get update</span></code></pre></td></tr></table></div></figure>


<p>Install docker:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install lxc-docker</span></code></pre></td></tr></table></div></figure>


<h2>Run the docker container</h2>

<p><strong>Find your nvidia devices</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -la /dev | grep nvidia</span></code></pre></td></tr></table></div></figure>


<p>You should see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>crw-rw-rw-  1 root root    195,   0 Oct 25 19:37 nvidia0
</span><span class='line'>crw-rw-rw-  1 root root    195, 255 Oct 25 19:37 nvidiactl
</span><span class='line'>crw-rw-rw-  1 root root    251,   0 Oct 25 19:37 nvidia-uvm</span></code></pre></td></tr></table></div></figure>


<p>You&rsquo;ll have to adapt the <code>DOCKER_NVIDIA_DEVICES</code> variable below to match your particular devices.</p>

<p>Here&rsquo;s how to start the docker container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ DOCKER_NVIDIA_DEVICES="--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm"
</span><span class='line'>$ sudo docker run -ti $DOCKER_NVIDIA_DEVICES tleyden5iwx/caffe-gpu /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s a large docker image, so this might take a few minutes, depending on your network connection.</p>

<h2>Run caffe test suite</h2>

<p>After the above <code>docker run</code> command completes, your shell will now be inside a docker container that has Caffe installed.</p>

<p>You&rsquo;ll want run the Caffe test suite and make sure it passes.  This will validate your environment, including your GPU drivers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /opt/caffe
</span><span class='line'>$ make test && make runtest</span></code></pre></td></tr></table></div></figure>


<p><strong>Expected Result:</strong> <code>... [  PASSED  ] 838 tests.</code></p>

<h2>Run the MNIST LeNet example</h2>

<p>A more comprehensive way to verify your environment is to train the MNIST LeNet example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /opt/caffe/data/mnist
</span><span class='line'>$ ./get_mnist.sh
</span><span class='line'>$ cd /opt/caffe
</span><span class='line'>$ ./examples/mnist/create_mnist.sh
</span><span class='line'>$ ./examples/mnist/train_lenet.sh</span></code></pre></td></tr></table></div></figure>


<p>This will take a few minutes.</p>

<p><strong>Expected output:</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>libdc1394 error: Failed to initialize libdc1394 
</span><span class='line'>I1018 17:02:23.552733    66 caffe.cpp:90] Starting Optimization 
</span><span class='line'>I1018 17:02:23.553583    66 solver.cpp:32] Initializing solver from parameters:
</span><span class='line'>... lots of output ...
</span><span class='line'>I1018 17:17:58.684598    66 caffe.cpp:102] Optimization Done.</span></code></pre></td></tr></table></div></figure>


<p>Congratulations, you&rsquo;ve got GPU-powered Caffe running in a docker container &mdash; celebrate with a cup of <a href="http://www.yelp.com/biz/philz-coffee-berkeley-2">Philz</a>!</p>

<h1>References</h1>

<ul>
<li><a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe-gpu">tleyden5iwx/caffe-gpu</a> Caffe Docker image (GPU)</li>
<li><a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe">tleyden5iwx/caffe</a> Caffe Docker image (CPU-only)</li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a></li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">CUDA 6.5 on AWS GPU Instance Running Ubuntu 14.04</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-25T13:25:00-07:00" pubdate data-updated="true">Oct 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Architecture</h2>

<p>After going through the steps in this blog post, you&rsquo;ll end up with this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/docker_gpu_aws_onion.png" alt="architecture diagram" /></p>

<h2>Setup host</h2>

<p>Before you can start your docker container, you will need to go <strong>deeper down the rabbit hole</strong>.</p>

<p>You&rsquo;ll first need to complete the steps here:</p>

<p><a href="http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">Setting up an Ubuntu 14.04 box running on a GPU-enabled AWS instance</a></p>

<p>After you&rsquo;re done, you&rsquo;ll end up with a host OS with the following properties:</p>

<ul>
<li>A GPU enabled AWS instance running Ubuntu 14.04</li>
<li>Nvidia kernel module</li>
<li>Nvidia device drivers</li>
<li>CUDA 6.5 installed and verified</li>
</ul>


<h2>Install Docker</h2>

<p>Once your host OS is setup, you&rsquo;re ready to install docker.  (version 1.3 at the time of this writing)</p>

<p>Setup the key for the docker repo:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9</span></code></pre></td></tr></table></div></figure>


<p>Add the docker repo:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo sh -c "echo deb https://get.docker.com/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list"
</span><span class='line'>$ sudo apt-get update</span></code></pre></td></tr></table></div></figure>


<p>Install docker:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install lxc-docker</span></code></pre></td></tr></table></div></figure>


<h2>Run GPU enabled docker image</h2>

<p><strong>Find all your nvidia devices</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -la /dev | grep nvidia</span></code></pre></td></tr></table></div></figure>


<p>You should see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>crw-rw-rw-  1 root root    195,   0 Oct 25 19:37 nvidia0
</span><span class='line'>crw-rw-rw-  1 root root    195, 255 Oct 25 19:37 nvidiactl
</span><span class='line'>crw-rw-rw-  1 root root    251,   0 Oct 25 19:37 nvidia-uvm</span></code></pre></td></tr></table></div></figure>


<p><strong>Launch docker container</strong></p>

<p>The easiest way to get going is to use this pre-built <a href="https://registry.hub.docker.com/u/tleyden5iwx/ubuntu-cuda/">docker image</a> that has the cuda drivers pre-installed.  Or if you want to build your own, <a href="https://registry.hub.docker.com/u/tleyden5iwx/ubuntu-cuda/dockerfile/">the accompanying dockerfile</a> will be a useful starting point.</p>

<p>You&rsquo;ll have to adapt the <code>DOCKER_NVIDIA_DEVICES</code> variable below to match your particular devices.</p>

<p>To start the docker container, run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ DOCKER_NVIDIA_DEVICES="--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm"
</span><span class='line'>$ sudo docker run -ti $DOCKER_NVIDIA_DEVICES tleyden5iwx/ubuntu-cuda /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>After running the above command, you should be at a shell inside your docker container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>root@1149788c731c:# </span></code></pre></td></tr></table></div></figure>


<h2>Verify CUDA access from inside the docker container</h2>

<p><strong>Install CUDA samples</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /opt/nvidia_installers
</span><span class='line'>$ ./cuda-samples-linux-6.5.14-18745345.run -noprompt -cudaprefix=/usr/local/cuda-6.5/</span></code></pre></td></tr></table></div></figure>


<p><strong>Build deviceQuery sample</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
</span><span class='line'>$ make
</span><span class='line'>$ ./deviceQuery   </span></code></pre></td></tr></table></div></figure>


<p><strong>You should see the following output</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
</span><span class='line'>Result = PASS</span></code></pre></td></tr></table></div></figure>


<h2>References</h2>

<ul>
<li><a href="https://registry.hub.docker.com/u/tleyden5iwx/ubuntu-cuda/">https://registry.hub.docker.com/u/tleyden5iwx/ubuntu-cuda/</a></li>
<li><a href="http://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container">http://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container</a></li>
<li><a href="http://docs.docker.com/installation/ubuntulinux/#ubuntu-trusty-1404-lts-64-bit">http://docs.docker.com/installation/ubuntulinux/#ubuntu-trusty-1404-lts-64-bit</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">CUDA 6.5 on AWS GPU Instance Running Ubuntu 14.04</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-25T11:56:00-07:00" pubdate data-updated="true">Oct 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Using a pre-built public AMI</h2>

<p>Based on the instructions in this blog post, I&rsquo;ve created an AMI and shared it publicly.  So the easiest thing to do is just use that pre-built AMI:</p>

<ul>
<li>Image: ami-2cbf3e44 (Ubuntu Server 14.04 LTS (HVM) &ndash; CUDA 6.5)</li>
<li>Instance type: g2.2xlarge</li>
<li>Storage: Use at least 8 GB, 20+ GB recommended</li>
</ul>


<p>If you use the pre-built AMI, then you can skip the rest of this article, since all of these steps are &ldquo;baked in&rdquo; to the AMI.</p>

<h2>Building from scratch</h2>

<p>Or if you prefer to build your own instance from scratch, keep reading.</p>

<p>Create a new EC2 instance:</p>

<ul>
<li>Image: ami-9eaa1cf6 (Ubuntu Server 14.04 LTS (HVM), SSD Volume Type)</li>
<li>Instance type: g2.2xlarge</li>
<li>Storage: Use at least 8 GB, 20+ GB recommended</li>
</ul>


<p>Install build-essential:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get update && apt-get install build-essential</span></code></pre></td></tr></table></div></figure>


<p>Get CUDA installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run</span></code></pre></td></tr></table></div></figure>


<p>Extract CUDA installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ chmod +x cuda_6.5.14_linux_64.run
</span><span class='line'>$ mkdir nvidia_installers
</span><span class='line'>$ ./cuda_6.5.14_linux_64.run -extract=`pwd`/nvidia_installers</span></code></pre></td></tr></table></div></figure>


<p>Run Nvidia driver installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd nvidia_installers
</span><span class='line'>$ ./NVIDIA-Linux-x86_64-340.29.run</span></code></pre></td></tr></table></div></figure>


<p>At this point it will popup an 8-bit UI that will ask you to accept a license agreement, and then start installing.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/install_cuda.png" alt="screenshot" /></p>

<p>At this point, I got an error:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Unable to load the kernel module 'nvidia.ko'.  This happens most frequently when this kernel module was built against the wrong or
</span><span class='line'>         improperly configured kernel sources, with a version of gcc that differs from the one used to build the target kernel, or if a driver
</span><span class='line'>         such as rivafb, nvidiafb, or nouveau is present and prevents the NVIDIA kernel module from obtaining ownership of the NVIDIA graphics
</span><span class='line'>         device(s), or no NVIDIA GPU installed in this system is supported by this NVIDIA Linux graphics driver release.
</span><span class='line'>
</span><span class='line'>         Please see the log entries 'Kernel module load error' and 'Kernel messages' at the end of the file '/var/log/nvidia-installer.log'
</span><span class='line'>         for more information.</span></code></pre></td></tr></table></div></figure>


<p>After <a href="https://devtalk.nvidia.com/default/topic/547588/error-installing-nvidia-drivers-on-x86_64-amazon-ec2-gpu-cluster-t20-gpu-/">reading this forum post</a> I installed:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install linux-image-extra-virtual</span></code></pre></td></tr></table></div></figure>


<p>When it prompted me what do to about the grub changes, I chose &ldquo;choose package maintainers version&rdquo;.</p>

<p>Reboot:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ reboot</span></code></pre></td></tr></table></div></figure>


<h2>Disable nouveau</h2>

<p>At this point you need to disable nouveau, since it conflicts with the nvidia kernel module.</p>

<p>Open a new file</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ vi /etc/modprobe.d/blacklist-nouveau.conf</span></code></pre></td></tr></table></div></figure>


<p>and add these lines to it</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blacklist nouveau
</span><span class='line'>blacklist lbm-nouveau
</span><span class='line'>options nouveau modeset=0
</span><span class='line'>alias nouveau off
</span><span class='line'>alias lbm-nouveau off</span></code></pre></td></tr></table></div></figure>


<p>and then save the file.</p>

<p>Disable the Kernel Nouveau:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf</span></code></pre></td></tr></table></div></figure>


<p>Reboot:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ update-initramfs -u
</span><span class='line'>$ reboot</span></code></pre></td></tr></table></div></figure>


<h2>One more try &mdash; this time it works</h2>

<p>Get Kernel source:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ apt-get install linux-source
</span><span class='line'>$ apt-get install linux-headers-3.13.0-37-generic
</span></code></pre></td></tr></table></div></figure>


<p>Rerun Nvidia driver installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd nvidia_installers
</span><span class='line'>$ ./NVIDIA-Linux-x86_64-340.29.run</span></code></pre></td></tr></table></div></figure>


<p>Load nvidia kernel module:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ modprobe nvidia</span></code></pre></td></tr></table></div></figure>


<p>Run CUDA + samples installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ./cuda-linux64-rel-6.5.14-18749181.run
</span><span class='line'>$ ./cuda-samples-linux-6.5.14-18745345.run</span></code></pre></td></tr></table></div></figure>


<h2>Verify CUDA is correctly installed</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
</span><span class='line'>$ make
</span><span class='line'>$ ./deviceQuery   </span></code></pre></td></tr></table></div></figure>


<p>You should see the following output:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
</span><span class='line'>Result = PASS</span></code></pre></td></tr></table></div></figure>


<h2>References</h2>

<ul>
<li><a href="http://www.r-tutor.com/gpu-computing/cuda-installation/cuda6.5-ubuntu">http://www.r-tutor.com/gpu-computing/cuda-installation/cuda6.5-ubuntu</a></li>
<li><a href="http://askubuntu.com/questions/451672/installing-and-testing-cuda-in-ubuntu-14-04">http://askubuntu.com/questions/451672/installing-and-testing-cuda-in-ubuntu-14-04</a></li>
<li><a href="https://devtalk.nvidia.com/default/topic/547588/error-installing-nvidia-drivers-on-x86_64-amazon-ec2-gpu-cluster-t20-gpu-/">https://devtalk.nvidia.com/default/topic/547588/error-installing-nvidia-drivers-on-x86_64-amazon-ec2-gpu-cluster-t20-gpu-/</a></li>
<li><a href="https://devtalk.nvidia.com/default/topic/769719/drm-ko-missing-on-ubuntu-14-04-1-lts-aws-ec2-g2-2xlarge-instance/">https://devtalk.nvidia.com/default/topic/769719/drm-ko-missing-on-ubuntu-14-04-1-lts-aws-ec2-g2-2xlarge-instance/</a></li>
<li><a href="http://askubuntu.com/questions/451221/ubuntu-14-04-install-nvidia-driver">http://askubuntu.com/questions/451221/ubuntu-14-04-install-nvidia-driver</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/10/debugging-into-android-source/">Debugging Into Android Source</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-10T15:57:00-07:00" pubdate data-updated="true">Sep 10<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Debugging into the core Android source code can be useful.  Here&rsquo;s how to do it in Android Studio 0.8.2.</p>

<p>Starting out, if we hit a breakpoint where we have a sqlite database object:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/outside_db_breakpoint.png" alt="screenshot" /></p>

<p>And if you step in, you get this, which isn&rsquo;t very useful:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/inside_db_breakpoint.png" alt="screenshot" /></p>

<p>To fix that, go to Android SDK, find the API level you are using, and check the Sources for Android SDK box.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/android_sdk.png" alt="screenshot" /></p>

<p><strong>You must restart Android Studio at this point</strong></p>

<p>Did you restart Android Studio?  Now re-run your app in the debugger, and when you try to step into the <code>database.execSQL()</code> method, you should see this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/db_source_breakpoint.png" alt="screenshot" /></p>

<p>It worked!  Now you can debug into any Android code.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>
  I&#8217;m a software engineer at Couchbase, working on <a href="https://github.com/couchbase/couchbase-lite-android">Couchbase Lite for Android</a> &#8211; a mobile NoSQL database with Sync.
  </p>

  <p>
  In my spare time I am working on <a href="http://www.openocr.net">OpenOCR</a> &#8211; a Dockerized REST API wrapper for Tesseract OCR.
  </p>

</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/12/02/getting-started-with-go-and-protocol-buffers/">Getting Started With Go and Protocol Buffers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/14/running-cbfs/">Running a CBFS Cluster on CoreOS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/12/an-example-of-using-nsq-from-go/">An Example of Using NSQ From Go</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/">CoreOS With Nvidia CUDA GPU Drivers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/">Running Couchbase Cluster Under CoreOS on AWS</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/tleyden">@tleyden</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'tleyden',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
     <h1>Twitter</h1>
<a class="twitter-timeline" width="300" height="350" href="https://twitter.com/tleydn" data-widget-id="376419812865368064">Tweets by @tleydn</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

 </section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Traun Leyden -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sevenstoryrabbithole';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>

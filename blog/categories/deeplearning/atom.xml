<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: deeplearning, | Seven Story Rabbit Hole]]></title>
  <link href="http://tleyden.github.io/blog/categories/deeplearning/atom.xml" rel="self"/>
  <link href="http://tleyden.github.io/"/>
  <updated>2016-05-19T23:25:30+00:00</updated>
  <id>http://tleyden.github.io/</id>
  <author>
    <name><![CDATA[Traun Leyden]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Running Neural Style on an AWS GPU instance]]></title>
    <link href="http://tleyden.github.io/blog/2015/11/22/running-neural-style-on-an-aws-gpu-instance/"/>
    <updated>2015-11-22T11:02:00+00:00</updated>
    <id>http://tleyden.github.io/blog/2015/11/22/running-neural-style-on-an-aws-gpu-instance</id>
    <content type="html"><![CDATA[<p>These instructions will walk you through getting <a href="https://github.com/jcjohnson/neural-style">neural-style</a> up and running on an AWS GPU instance.</p>

<h2>Spin up CUDA-enabled AWS instance</h2>

<p>Follow these instructions to <a href="http://tleyden.github.io/blog/2015/11/22/cuda-7-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/">install CUDA 7.5 on AWS GPU Instance Running Ubuntu 14.04</a>.</p>

<h2>SSH into AWS instance</h2>

<p><code>
$ ssh ubuntu@&lt;instance-ip&gt;
</code></p>

<h2>Install Docker</h2>

<p><code>
$ sudo apt-get update &amp;&amp; sudo apt-get install curl
$ curl -sSL https://get.docker.com/ | sh
</code></p>

<p>As the post-install message suggests, enable docker for non-root users:</p>

<p><code>
$ sudo usermod -aG docker ubuntu
</code></p>

<p>Verify correct install via:</p>

<p><code>
$ sudo docker run hello-world
</code></p>

<h2>Mount GPU devices</h2>

<p><strong>Mount</strong></p>

<p><code>
$ cd /usr/local/cuda/samples/1_Utilities/deviceQuery
$ sudo make
$ sudo ./deviceQuery
</code></p>

<p>You should see something <a href="https://gist.github.com/tleyden/58ab2eedebc9529edb76">like this</a>:</p>

<p>```
./deviceQuery Starting&hellip;</p>

<p> CUDA Device Query (Runtime API) version (CUDART static linking)</p>

<p>Detected 1 CUDA Capable device(s)</p>

<p>Device 0: &ldquo;GRID K520&rdquo;
  CUDA Driver Version / Runtime Version          6.5 / 6.5
  &hellip; snip &hellip;</p>

<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
Result = PASS
```</p>

<p><strong>Verify: Find all your nvidia devices</strong></p>

<p><code>
$ ls -la /dev | grep nvidia
</code></p>

<p>You should see:</p>

<p><code>
crw-rw-rw-  1 root root    195,   0 Oct 25 19:37 nvidia0
crw-rw-rw-  1 root root    195, 255 Oct 25 19:37 nvidiactl
crw-rw-rw-  1 root root    251,   0 Oct 25 19:37 nvidia-uvm
</code></p>

<h2>Start Docker container</h2>

<p><code>
$ export DOCKER_NVIDIA_DEVICES="--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm"
$ sudo docker run -ti $DOCKER_NVIDIA_DEVICES kaixhin/cuda-torch /bin/bash
</code></p>

<h2>Re-install CUDA 7.5 in the Docker container</h2>

<p>As <a href="https://groups.google.com/d/msg/torch7/yCSNIzW590M/Af7CHXEdDQAJ">reported in the Torch7 Google Group</a> and in <a href="https://github.com/Kaixhin/dockerfiles/issues/6">Kaixhin/dockerfiles</a>, there is an API version mismatch with the docker container and the host&rsquo;s version of CUDA.</p>

<p>The workaround is to re-install CUDA 7.5 via:</p>

<p>```
$ wget <a href="http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_7.5-18_amd64.deb">http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_7.5-18_amd64.deb</a>
$ sudo dpkg -i cuda-repo-ubuntu1404_7.5-18_amd64.
deb
$ sudo apt-get update
$ sudo apt-get upgrade -y
$ sudo apt-get install -y opencl-headers build-essential protobuf-compiler \</p>

<pre><code>libprotoc-dev libboost-all-dev libleveldb-dev hdf5-tools libhdf5-serial-dev \
libopencv-core-dev  libopencv-highgui-dev libsnappy-dev libsnappy1 \
libatlas-base-dev cmake libstdc++6-4.8-dbg libgoogle-glog0 libgoogle-glog-dev \
libgflags-dev liblmdb-dev git python-pip gfortran
</code></pre>

<p>$ sudo apt-get clean
$ sudo apt-get install -y linux-image-extra-<code>uname -r</code> linux-headers-<code>uname -r</code> linux-image-<code>uname -r</code>
$ sudo apt-get install -y cuda
```</p>

<h2>Verify CUDA inside docker container</h2>

<p>Running:</p>

<p><code>
$ nvidia-smi
</code></p>

<p>Should show info about the GPU driver and not return any errors.</p>

<p>Running this torch command:</p>

<p><code>
$ th -e "require 'cutorch'; require 'cunn'; print(cutorch)"
</code></p>

<p>Should produce this output:</p>

<p><code>
{
  getStream : function: 0x4054b760
  getDeviceCount : function: 0x408bca58
  .. etc
}
</code></p>

<h2>Install neural-style</h2>

<p>The following should be run <strong>inside</strong> the docker container:</p>

<p><code>
$ apt-get install -y wget libpng-dev libprotobuf-dev protobuf-compiler
$ git clone --depth 1 https://github.com/jcjohnson/neural-style.git
$ /root/torch/install/bin/luarocks install loadcaffe
</code></p>

<p><strong>Download models</strong></p>

<p><code>
$ cd neural-style
$ sh models/download_models.sh
</code></p>

<h2>Run neural style</h2>

<p>First, grab a few images to test with</p>

<p><code>
$ mkdir images
$ wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -O images/vangogh.jpg
$ wget http://exp.cdn-hotels.com/hotels/1000000/10000/7500/7496/7496_42_z.jpg -O images/hotel_del_coronado.jpg
</code></p>

<p>Run it:</p>

<p><code>
$ th neural_style.lua -style_image images/vangogh.jpg -content_image images/hotel_del_coronado.jpg
</code></p>

<h2>CuDNN (optional)</h2>

<p>CuDNN can potentially speed things up.</p>

<p><a href="https://developer.nvidia.com/cudnn">download cuDNN</a></p>

<p>Install via:</p>

<p><code>
tar -xzvf cudnn-7.0-linux-x64-v3.0-prod.tgz
cd cuda/
sudo cp lib64/libcudnn* /usr/local/cuda-7.5/lib64/
sudo cp include/cudnn.h /usr/local/cuda-7.5/include
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.5/lib64/
</code></p>

<p>Install the torch bindings for cuDNN:</p>

<p><code>
luarocks install cudnn
</code></p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/jcjohnson/neural-style/blob/master/INSTALL.md">Neural-Style INSTALL.md</a></li>
<li>ami-84c787ee &mdash; this AMI has everything pre-installed, however it is installed on the host rather than under docker, which was due to time constraints.</li>
</ul>

]]></content>
  </entry>
  
</feed>

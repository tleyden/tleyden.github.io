<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Seven Story Rabbit Hole]]></title>
  <link href="http://tleyden.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tleyden.github.io/"/>
  <updated>2015-11-03T12:15:17-08:00</updated>
  <id>http://tleyden.github.io/</id>
  <author>
    <name><![CDATA[Traun Leyden]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Running Couchbase Server under Joyent Triton]]></title>
    <link href="http://tleyden.github.io/blog/2015/05/05/running-couchbase-server-under-docker-on-joyent/"/>
    <updated>2015-05-05T09:31:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2015/05/05/running-couchbase-server-under-docker-on-joyent</id>
    <content type="html"><![CDATA[<p>Joyent has recently announced their new Triton Docker container hosting service.  There are several advantages of running Docker containers on Triton over a more traditional cloud hosting platform:</p>

<ul>
<li><p>Better performance since there is no hardware level virtualization overhead.  Your containers run on bare-metal.</p></li>
<li><p>Simplified networking between containers.  Each container gets its own private (and optionally public) ip address.</p></li>
<li><p>Hosts are abstracted away &mdash; you just deploy into the &ldquo;container cloud&rdquo;, and don&rsquo;t care which host your container is running on.</p></li>
</ul>


<p>For more details, check out Bryan Cantrill&rsquo;s talk about <a href="https://www.joyent.com/developers/videos/docker-and-the-future-of-containers-in-production">Docker and the Future of Containers in Production</a>.</p>

<p>Let&rsquo;s give it a spin with a &ldquo;hello world&rdquo; container, and then with a cluster of Couchbase servers.</p>

<h2>Sign up for a Joyent account</h2>

<p><a href="https://www.joyent.com/lp/preview">Follow the signup instructions on the Joyent website</a></p>

<p>You will also need to add your SSH key to your account.</p>

<h2>Install or Upgrade Docker</h2>

<p>If you don&rsquo;t have Docker installed already and you are on Ubuntu, run:</p>

<p><code>
$ curl -sSL https://get.docker.com/ | sh
</code></p>

<p>See <a href="https://docs.docker.com/installation/ubuntulinux/">install Docker on Ubuntu</a> for more details.</p>

<h2>Upgrade Docker client to 1.4.1 or later</h2>

<p>Check your version of Docker with:</p>

<p><code>
$ docker --version
Docker version 1.0.1, build 990021a
</code></p>

<p>If you are on a version before 1.4.1 (like I was), you can upgrade Docker via the <a href="https://github.com/boot2docker/osx-installer/releases">boot2docker installers</a>.</p>

<h2>Joyent + Docker setup</h2>

<p>Get the sdc-docker repo (sdc == Smart Data Center):</p>

<p><code>
$ git clone https://github.com/joyent/sdc-docker.git
</code></p>

<p>Perform setup via:</p>

<p><code>
$ cd sdc-docker
$  ./tools/sdc-docker-setup.sh -k 165.225.168.22 $ACCOUNT ~/.ssh/$PRIVATE_KEY_FILE
</code></p>

<p>Replace values as follows:</p>

<ul>
<li><strong>$ACCOUNT</strong>: you can get this by logging into the Joyent web ui and going to the Account menu from the pulldown in the top-right corner.  Find the <strong>Username</strong> field, and use that</li>
<li><strong>$PRIVATE_KEY_FILE</strong>: the name of the file where your private key is stored, typically this will be <code>id_rsa</code></li>
</ul>


<p>Run the command and you should see the following output:</p>

<p>```
Setting up Docker client for SDC using:</p>

<pre><code>CloudAPI:        https://165.225.168.22
Account:         &lt;your username&gt;
Key:             /home/ubuntu/.ssh/id_rsa
</code></pre>

<p>[..snip..]</p>

<p>Wrote certificate files to /home/ubuntu/.sdc/docker/<username></p>

<p>Docker service endpoint is: tcp://<generated ip>:2376</p>

<hr />

<p>Success. Set your environment as follows:</p>

<pre><code>export DOCKER_CERT_PATH=/home/ubuntu/.sdc/docker/&lt;username&gt;
export DOCKER_HOST=tcp://&lt;generated-ip&gt;:2376
alias docker="docker --tls"
</code></pre>

<p>Then you should be able to run &lsquo;docker info&rsquo; and see your account
name &lsquo;SDCAccount: <username>&rsquo; in the output.
```</p>

<p><strong>Export environment variables</strong></p>

<p>As the output above suggests, copy and paste the commands from the output.  Here&rsquo;s an example of what that will look like (but you should copy and paste from your command output, not the snippet below):</p>

<p><code>
$ export DOCKER_CERT_PATH=/home/ubuntu/.sdc/docker/&lt;username&gt;
$ export DOCKER_HOST=tcp://&lt;generated-ip&gt;:2376
$ alias docker="docker --tls"
</code></p>

<h2>Docker Hello World</h2>

<p>Let&rsquo;s spin up an Ubuntu docker image that says hello world.</p>

<p>Remember you&rsquo;re running the Docker client on your workstation, not in the cloud.  Here&rsquo;s an overview on what&rsquo;s going to be happening:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/joyent_container_hello_world.png" alt="diagram" /></p>

<p>To start the docker container::</p>

<p><code>
$ docker run --rm ubuntu:14.04 echo "Hello Docker World, from Joyent"
</code></p>

<p>You should see the following output:</p>

<p><code>
Unable to find image 'ubuntu:14.04' locally
Pulling repository library/ubuntu
...
Hello Docker World, from Joyent
</code></p>

<p>Also, since the <code>--rm</code> flag was passed, the container will have been removed after exiting.  You can verify this by running <code>docker ps -a</code>.  This is important because <strong>stopped containers incur charges on Joyent</strong>.</p>

<p>Congratulations!  You&rsquo;ve gotten a &ldquo;hello world&rdquo; Docker container running on Joyent.</p>

<h2>Run Couchbase Server containers</h2>

<p>Now it&rsquo;s time to run Couchbase Server.</p>

<p>To kick off three Couchbase Server containers, run:</p>

<p><code>``
$ for i in</code>seq 1 3`; do \</p>

<pre><code>  echo "Starting container $i"; \
  export container_$i=$(docker run --name couchbase-server-$i -d -P couchbase/server); \
</code></pre>

<p>  done
```</p>

<p>To confirm the containers are up, run:</p>

<p><code>
$ docker ps
</code></p>

<p>and you should see:</p>

<p><code>
CONTAINER ID        IMAGE                                       COMMAND             CREATED             STATUS              PORTS               NAMES
5bea8901814c        couchbase/server   "couchbase-start"   3 minutes ago       Up 2 minutes                            couchbase-server-1
bef1f2f32726        couchbase/server   "couchbase-start"   2 minutes ago       Up 2 minutes                            couchbase-server-2
6f4e2a1e8e63        couchbase/server   "couchbase-start"   2 minutes ago       Up About a minute                       couchbase-server-3
</code></p>

<p>At this point you will have environment variables defined with the container ids of each container.  You can check this by running:</p>

<p><code>
$ echo $container_1 &amp;&amp; echo $container_2 &amp;&amp; echo $container_3
21264e44d66b4004b4828b7ae408979e7f71924aadab435aa9de662024a37b0e
ff9fb4db7b304e769f694802e6a072656825aa2059604ba4ab4d579bd2e5d18d
0c6f8ca2951448e497d7e12026dcae4aeaf990ec51e047cf9d8b2cbdd9bd7668
</code></p>

<h3>Get public ip addresses of the containers</h3>

<p>Each container will have two IP addresses assigned:</p>

<ul>
<li>A public IP, accessible from anywhere</li>
<li>A private IP, only accessible from containers/machines in your Joyent account</li>
</ul>


<p>To get the public IP, we can use the Docker client.  (to get the private IP, you need to use the Joyent SmartDataCenter tools, which is described below)</p>

<p><code>``
$ container_1_ip=</code>docker inspect $container_1 | grep -i IPAddress | awk -F: &lsquo;{print $2}&rsquo; |  grep -oE &ldquo;\b([0-9]{1,3}.){3}[0-9]{1,3}\b&rdquo;<code>
$ container_2_ip=</code>docker inspect $container_2 | grep -i IPAddress | awk -F: &lsquo;{print $2}&rsquo; |  grep -oE &ldquo;\b([0-9]{1,3}.){3}[0-9]{1,3}\b&rdquo;<code>
$ container_3_ip=</code>docker inspect $container_3 | grep -i IPAddress | awk -F: &lsquo;{print $2}&rsquo; |  grep -oE &ldquo;\b([0-9]{1,3}.){3}[0-9]{1,3}\b&rdquo;`</p>

<p>```</p>

<p>You will now have the public IP addresses of each container defined in environment variables.  You can check that it worked via:</p>

<p><code>
$ echo $container_1_ip &amp;&amp; echo $container_2_ip &amp;&amp; echo $container_3_ip
165.225.185.11
165.225.185.12
165.225.185.13
</code></p>

<h3>Connect to Couchbase Web UI</h3>

<p>Open your browser to $container_1_ip:8091 and you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_cluster_setup.png" alt="Couchbase Welcome Screen" /></p>

<p>At this point, it&rsquo;s possible to setup the cluster by going to each Couchbase node&rsquo;s Web UI and following the Setup Wizard.  However, in case you want to automate this in the future, let&rsquo;s do this over the command line instead.</p>

<h3>Setup first Couchbase node</h3>

<p>Let&rsquo;s arbitrarily pick <strong>container_1</strong> as the first node in the cluster.  This node is special in the sense that other nodes will join it.</p>

<p>The following command will do the following:</p>

<ul>
<li>Set the Administrator&rsquo;s username and password to Administrator / password (you should change this)</li>
<li>Set the cluster RAM size to 600 MB</li>
</ul>


<p>Note: the <code>-u admin -p password</code> should be left as-is, since that is just passing in the default admin name and password for auth purposes.</p>

<p><code>
$ docker run --rm --entrypoint=/opt/couchbase/bin/couchbase-cli couchbase/server \
cluster-init -c $container_1_ip \
--cluster-init-username=Administrator \
--cluster-init-password=password \
--cluster-init-ramsize=600 \
-u admin -p password
</code></p>

<p>You should see a response like:</p>

<p><code>
SUCCESS: init 165.225.185.11
</code></p>

<h3>Create a default bucket</h3>

<p>A bucket is equivalent to a database in typical RDMS systems.</p>

<p><code>
$ docker run --rm --entrypoint=/opt/couchbase/bin/couchbase-cli couchbase/server \
bucket-create -c $container_1_ip:8091 \
--bucket=default \
--bucket-type=couchbase \
--bucket-port=11211 \
--bucket-ramsize=600 \
--bucket-replica=1 \
-u Administrator -p password
</code></p>

<p>You should see:</p>

<p><code>
SUCCESS: bucket-create
</code></p>

<h3>Add 2nd Couchbase node</h3>

<p>Add in the second Couchbase node with this command</p>

<p><code>
$ docker run --rm --entrypoint=/opt/couchbase/bin/couchbase-cli couchbase/server \
server-add -c $container_1_ip \
-u Administrator -p password \
--server-add $container_2_ip \
--server-add-username Administrator \
--server-add-password password
</code></p>

<p>You should see:</p>

<p><code>
SUCCESS: server-add 165.225.185.12:8091
</code></p>

<p>To verify it was added, run:</p>

<p><code>
$ docker run --rm --entrypoint=/opt/couchbase/bin/couchbase-cli couchbase/server \
server-list -c $container_1_ip \
-u Administrator -p password
</code></p>

<p>which should return the list of Couchbase Server nodes that are now part of the cluster:</p>

<p><code>
ns_1@165.225.185.11 165.225.185.11:8091 healthy active
ns_1@165.225.185.12 165.225.185.12:8091 healthy inactiveAdded
</code></p>

<h3>Add 3rd Couchbase node and rebalance</h3>

<p>In this step we will:</p>

<ul>
<li>Add the 3rd Couchbase node</li>
<li>Trigger a &ldquo;rebalance&rdquo;, which distributes the (empty) bucket&rsquo;s data across the cluster</li>
</ul>


<p><code>
$ docker run --rm --entrypoint=/opt/couchbase/bin/couchbase-cli couchbase/server \
rebalance -c $container_1_ip \
-u Administrator -p password \
--server-add $container_3_ip \
--server-add-username Administrator \
--server-add-password password
</code></p>

<p>You should see:</p>

<p>```
INFO: rebalancing &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip; &hellip;
SUCCESS: rebalanced cluster
close failed in file object destructor:
Error in sys.excepthook:</p>

<p>Original exception was:
```</p>

<p>If you see <strong>SUCCESS</strong>, then it worked.  <em>(I&rsquo;m not sure why the &ldquo;close failed in file ..&rdquo; error is happening, but so far it appears that it can be safely ignored.)</em></p>

<h3>Login to Web UI</h3>

<p>Open your browser to $container_1_ip:8091 and you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_cluster_login.png" alt="Couchbase Login Screen" /></p>

<p>Login with:</p>

<ul>
<li>Username: <strong>Administrator</strong></li>
<li>Password: <strong>password</strong></li>
</ul>


<p>And you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/couchbase_cluster_nodes.png" alt="Couchbase Nodes" /></p>

<p>Congratulations!  You have a Couchbase Server cluster up and running on Joyent Triton.</p>

<h2>Teardown</h2>

<p>To stop and remove your Couchbase server containers, run:</p>

<p><code>
$ docker stop $container_1 $container_2 $container_3
$ docker rm $container_1 $container_2 $container_3
</code></p>

<p>To double check that you no longer have any containers running or in the stopped state, run <code>docker ps -a</code> and you should see an empty list.</p>

<h2>Installing the SDC tools (optional)</h2>

<p>Installing the Joyent Smart Data Center (SDC) tools will allow you to gain more visibility into your container cluster &mdash; for example being able to view the internal IP of each continer.</p>

<p>Here&rsquo;s how to install the sdc-tools suite.</p>

<h3>Install smartdc</h3>

<p>First <a href="http://coolestguidesontheplanet.com/installing-node-js-on-osx-10-10-yosemite/">install NodeJS + NPM</a></p>

<p>Install smartdc:</p>

<p><code>
npm install -g smartdc
</code></p>

<h3>Configure environment variables</h3>

<p><code>
$ export SDC_URL=https://us-east-3b.api.joyent.com
$ export SDC_ACCOUNT=&lt;ACCOUNT&gt;
$ export SDC_KEY_ID=$(ssh-keygen -l -f $HOME/.ssh/id_rsa.pub | awk '{print $2}')
</code></p>

<p>Replace values as follows:</p>

<ul>
<li><strong>ACCOUNT</strong>: you can get this by logging into the Joyent web ui and going to the Account menu from the pulldown in the top-right corner.  Find the <strong>Username</strong> field, and use that</li>
</ul>


<h3>List machines</h3>

<p>Run <code>sdc-listmachines</code> to list all the containers running under your Joyent account.  Your output should look something like this:</p>

<p>```
$ sdc-listmachines
[
{</p>

<pre><code>"id": "0c6f8ca2-9514-48e4-97d7-e12026dcae4a",
"name": "couchbase-server-3",
"type": "smartmachine",
"state": "running",
"image": "335a8046-0749-1174-5666-6f084472b5ef",
"ips": [
  "192.168.128.32",
  "165.225.185.13"
],
"memory": 1024,
"disk": 25600,
"metadata": {},
"tags": {},
"created": "2015-03-26T14:50:31.196Z",
"updated": "2015-03-26T14:50:45.000Z",
"networks": [
  "7cfe29d4-e313-4c3b-a967-a28ea34342e9",
  "178967cb-8d11-4f53-8434-9c91ff819a0d"
],
"dataset": "335a8046-0749-1174-5666-6f084472b5ef",
"primaryIp": "165.225.185.13",
"firewall_enabled": false,
"compute_node": "44454c4c-4400-1046-8050-b5c04f383432",
"package": "t4-standard-1G"
</code></pre>

<p>  },
]
```</p>

<h3>Find private IP of an individual machine</h3>

<p><code>
$ sdc-getmachine &lt;machine_id&gt; | json -aH ips | json -aH | egrep "10\.|192\.”
</code></p>

<h2>References</h2>

<ul>
<li><p><a href="https://github.com/joyent/sdc-docker/blob/master/docs/divergence.md">Native Docker API vs Joyent Triton API</a></p></li>
<li><p><a href="https://www.joyent.com/blog/container-service-preview">https://www.joyent.com/blog/container-service-preview</a></p></li>
<li><p><a href="https://www.joyent.com/blog/docker-bake-off-aws-vs-joyent">https://www.joyent.com/blog/docker-bake-off-aws-vs-joyent</a></p></li>
<li><p><a href="https://github.com/joyent/sdc-docker">https://github.com/joyent/sdc-docker</a></p></li>
<li><p><a href="https://github.com/joyent/sdc-docker/blob/master/docs/divergence.md">https://github.com/joyent/sdc-docker/blob/master/docs/divergence.md</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Octopress under Docker]]></title>
    <link href="http://tleyden.github.io/blog/2015/04/25/setting-up-octopress-under-docker/"/>
    <updated>2015-04-25T03:57:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2015/04/25/setting-up-octopress-under-docker</id>
    <content type="html"><![CDATA[<p>I got a new computer last week.  It&rsquo;s the latest macbook retina, and I needed to refresh because I wanted a bigger SSD drive (and after having an SSD drive, I&rsquo;ll never go back)</p>

<p>Anyway, I&rsquo;m trying to get my Octopress blog going again, and oh my God, what a nightmare.  Octopress was working beautifully for me for years, and then all of the sudden I am at the edge of Ruby Dependency Hell staring at an Octopress giving me eight fingers.</p>

<p>With the help of Docker, I&rsquo;ve managed to tame this eight legged beast, barely.</p>

<h2>Run Docker</h2>

<p>See <a href="https://docs.docker.com/installation/">Installing Docker</a> for instructions.</p>

<p>This blog post assumes you <strong>already have an Octopress git repo</strong>.  If you are starting from scratch, then check out <a href="http://tleyden.github.io/blog/2013/09/07/octopress-setup-part-i/">Octopress Setup Part I</a> to become even more confused.</p>

<h2>Install Octopress Docker image</h2>

<p><code>
$ docker run -ti tleyden5iwx/octopress /bin/bash
</code></p>

<p>After this point, the rest of the instructions assume that you are executing commands from inside the Docker Container.</p>

<h2>Delete Octopress dir + clone your Octopress repo</h2>

<p>The Docker container will contain an Octopress directory, but it&rsquo;s not needed.</p>

<p>From within the container:</p>

<p><code>
$ cd /root
$ rm -rf octopress/
$ git clone https://github.com/your-github-username/your-github-username.github.io.git octopress
$ cd octopress/
</code></p>

<p>Now, switch to the source branch (which contains the content)</p>

<p><code>
$ git checkout source
</code></p>

<p>Re-install dependencies:</p>

<p><code>
$ bundle install
</code></p>

<p>Prevent ASCII encoding errors:</p>

<p><code>
$ export LC_ALL=C.UTF-8
</code></p>

<p><strong>Clone deploy directory</strong></p>

<p><code>
$ git clone https://github.com/your-github-username/your-github-username.github.io.git _deploy
</code></p>

<h2>Rake preview</h2>

<p>As a smoke test, run:</p>

<p><code>
$ bundle exec rake preview
</code></p>

<p>NOTE: I have no idea why <code>bundle exec</code> is required here, I just used this in response to a previous error message and it&rsquo;s accompanying suggestion.</p>

<p>If this gives no errors, that&rsquo;s a good sign.</p>

<h2>Create a new blog post</h2>

<p><code>
$ bundle exec rake new_post["Setting up Octopress under Docker"]
</code></p>

<p>It will tell you the path to the blog post.  Now open the file in your favorite editor and add contect.</p>

<h2>Push to Source branch</h2>

<p>The source branch has the <strong>source markdown content</strong>.  It&rsquo;s actually the most important thing to preserve, because the HTML can always be regnerated from it.</p>

<p><code>
$ git push origin source
</code></p>

<h2>Deploy to Master branch</h2>

<p>The master branch contains the <strong>rendered HTML content</strong>.  Here&rsquo;s how to push it up to your github pages repo (remember, in an earlier step you cloned your github pages repo at <a href="https://github.com/your-github-username/your-github-username.github.io.git">https://github.com/your-github-username/your-github-username.github.io.git</a>)</p>

<p><code>
$ bundle exec rake generate &amp;&amp; bundle exec rake deploy
</code></p>

<p>After the above command, the changes should be visible on your github pages blog (eg, your-username.github.io)</p>

<h2>Common errors</h2>

<p>If you get:</p>

<p><code>
YAML Exception reading 2014-04-09-a-successful-git-branching-model-with-enterprise-support.markdown: invalid byte sequence in US-ASCII
</code></p>

<p>Run:</p>

<p><code>
$ export LC_ALL=C.UTF-8
</code></p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/imathis/octopress/issues/1344">https://github.com/imathis/octopress/issues/1344</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx proxy for Sync Gateway using Confd]]></title>
    <link href="http://tleyden.github.io/blog/2015/03/21/nginx-proxy-for-sync-gateway-using-confd/"/>
    <updated>2015-03-21T15:25:00-07:00</updated>
    <id>http://tleyden.github.io/blog/2015/03/21/nginx-proxy-for-sync-gateway-using-confd</id>
    <content type="html"><![CDATA[<p>This will walk you through setting up Sync Gateway behind nginx.  The nginx conf will be auto generated based on Sync Gateway status.</p>

<h3>Launch CoreOS instances on EC2</h3>

<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#cstack=sn%7ECouchbase-CoreOS%7Cturl%7Ehttp://tleyden-misc.s3.amazonaws.com/couchbase-coreos/sync_gateway.template"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png"></a></p>

<p>Recommended values:</p>

<ul>
<li><strong>ClusterSize</strong>: 3 nodes (default)</li>
<li><strong>Discovery URL</strong>:  as it says, you need to grab a new token from <a href="https://discovery.etcd.io/new">https://discovery.etcd.io/new</a> and paste it in the box.</li>
<li><strong>KeyPair</strong>: the name of the AWS keypair you want to use.  If you haven&rsquo;t already, you&rsquo;ll want to upload your local ssh key into AWS and create a named keypair.</li>
</ul>


<h3>Wait until instances are up</h3>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cloud-formation-create-complete.png" alt="screenshot" /></p>

<h3>ssh into a CoreOS instance</h3>

<p>Go to the AWS console under EC2 instances and find the public ip of one of your newly launched CoreOS instances.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/ec2-instances-coreos.png" alt="screenshot" /></p>

<p>Choose any one of them (it doesn&rsquo;t matter which), and ssh into it as the <strong>core</strong> user with the cert provided in the previous step:</p>

<p><code>
$ ssh -i aws.cer -A core@ec2-54-83-80-161.compute-1.amazonaws.com
</code></p>

<h2>Spin up Sync Gateway containers</h2>

<p><code>
$ etcdctl set /couchbase.com/enable-code-refresh true
$ sudo docker run --net=host tleyden5iwx/couchbase-cluster-go update-wrapper sync-gw-cluster launch-sgw --num-nodes=2 --config-url=http://git.io/hFwa --in-memory-db
</code></p>

<h2>Verify etcd entries</h2>

<p><code>
$ etcdctl ls --recursive /
...
/couchbase.com/sync-gw-node-state
/couchbase.com/sync-gw-node-state/10.169.70.114
/couchbase.com/sync-gw-node-state/10.231.220.110
</code></p>

<h2>Create data volume container</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/confdata.service
$ fleetctl start confdata.service
</code></p>

<h2>Launch sync-gateway-nginx-confd.service</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/confd.service
$ sed -i -e 's/lordelph\/confd-demo/tleyden5iwx\/sync-gateway-nginx-confd/' confd.service
$ fleetctl start confd.service
</code></p>

<h2>Launch nginx service</h2>

<p><code>
$ wget https://raw.githubusercontent.com/lordelph/confd-demo/master/nginx.service
$ fleetctl start nginx.service
</code></p>

<h2>Verify</h2>

<p>Try a basic http get.</p>

<p><code>
$ nginx_ip=`fleetctl list-units | grep -i nginx | awk '{print $2}' | awk -F/ '{print $2}'`
$ curl $nginx_ip
{"couchdb":"Welcome","vendor":{"name":"Couchbase Sync Gateway","version":1},"version":"Couchbase Sync Gateway/master(a47a17f)"}
</code></p>

<p>Add &lsquo;-v&rsquo; flag to see which Sync Gateway server is servicing the request</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.110:4984
...
</code></p>

<p>If you repeat that a few more times, you should see different ip addresses for the handler.</p>

<p><strong>Take a sync gateway out of rotation</strong></p>

<p><code>
$ fleetctl stop sync_gw_node@1.service sync_gw_sidekick@1.service
</code></p>

<p>Now try hitting nginx again, and should not see the Sync Gw that you just shutdown as a handler.</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.114:4984
...
</code></p>

<p><strong>Put sync gateway back into rotation</strong></p>

<p><code>
$ fleetctl start sync_gw_node@1.service sync_gw_sidekick@1.service
</code></p>

<p>Now try hitting nginx again, and should again see the Sync Gw that you just restarted as being a handler.</p>

<p><code>
$ curl -v $nginx_ip
...
X-Handler: 10.231.220.110:4984
...
</code></p>

<h2>References</h2>

<ul>
<li><a href="http://blog.dixo.net/2015/02/load-balancing-with-coreos">http://blog.dixo.net/2015/02/load-balancing-with-coreos</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running a CBFS cluster on CoreOS]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/14/running-cbfs/"/>
    <updated>2014-11-14T06:43:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/14/running-cbfs</id>
    <content type="html"><![CDATA[<p>This will walk you through getting a cbfs cluster up and running.</p>

<h2>What is CBFS?</h2>

<p>cbfs is a distributed filesystem on top of Couchbase Server, not unlike Mongo&rsquo;s GridFS or Riak&rsquo;s CS.</p>

<p>Here&rsquo;s a typical deployment architecture:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs-overview.png" alt="cbfs overview" /></p>

<p>Although not shown, all cbfs daemons can communicate with all Couchbase Server instances.</p>

<p>It is not required to run cbfs on the same machine as Couchbase Server, but it <em>is</em> meant to be run in the same data center as Couchbase Server.</p>

<p>If you want a deeper understanding of how cbfs works, check the <a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a> or this <a href="http://dustin.sallings.org/2012/09/27/cbfs.html">blog post</a>.</p>

<h2>Kick off a Couchbase Cluster</h2>

<p>cbfs depends on having a Couchbase cluster running.</p>

<p>Follow all of the steps in <a href="http://tleyden.github.io/blog/2014/11/01/running-couchbase-cluster-under-coreos-on-aws/">Running Couchbase Cluster Under CoreOS on AWS</a> to kick off a 3 node Couchbase cluster.</p>

<h2>Add security groups</h2>

<p>A few ports will need to be opened up for cbfs.</p>

<p>Go to the AWS console and edit the Couchbase-CoreOS-CoreOSSecurityGroup-xxxx security group and add the following rules:</p>

<p>```
Type             Protocol  Port Range Source</p>

<hr />

<p>Custom TCP Rule  TCP       8484       Custom IP: sg-6e5a0d04 (copy and paste from port 4001 rule)
Custom TCP Rule  TCP       8423       Custom IP: sg-6e5a0d04
```</p>

<p>At this point your security group should look like this:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/security_group_cbfs.png" alt="security group" /></p>

<h1>Create a new bucket for cbfs</h1>

<p><strong>Open Couchbase Server Admin UI</strong></p>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>In your browser, go to <code>http://&lt;public_ip&gt;:8091/</code></p>

<p><strong>Create Bucket</strong></p>

<p>Go to Data Buckets / Create New Bucket</p>

<p>Enter <strong>cbfs</strong> for the name of the bucket.</p>

<p>Leave all other settings as default.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/cbfs_create_bucket.png" alt="create bucket" /></p>

<h2>ssh in</h2>

<p>In the AWS EC2 console, find the public IP of one of the instances (it doesn&rsquo;t matter which)</p>

<p>ssh into one of the machines:</p>

<p><code>
$ ssh -A core@&lt;public_ip&gt;
</code></p>

<h2>Run cbfs</h2>

<p><strong>Create a volume dir</strong></p>

<p>Since the fileystem of a docker container is not meant for high throughput io, a volume should be used for cbfs.</p>

<p>Create a directory on the host OS (i.e., on the Core OS instance)</p>

<p><code>
$ sudo mkdir -p /var/lib/cbfs/data
$ sudo chown -R core:core /var/lib/cbfs
</code></p>

<p>This will be mounted by the docker container in the next step.</p>

<p><strong>Generate fleet unit files</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/d70161c3827cb8b788a8/raw/8f6c81f0095b0007565e9b205e90afb132552060/cbfs_node.service.template
$ for i in `seq 1 3`; do cp cbfs_node.service.template cbfs_node.$i.service; done
</code></p>

<p><strong>Start cbfs on all cluster nodes</strong></p>

<p><code>
$ fleetctl start cbfs_node.*.service
</code></p>

<p>Run <code>fleetctl list-units</code> to list the  units running in your cluster.  You should have the following:</p>

<p><code>
$ fleetctl list-units
UNIT                                            MACHINE                         ACTIVE  SUB
cbfs_node.1.service                             6ecff20c.../10.51.177.81        active  running
cbfs_node.2.service                             b8eb6653.../10.79.155.153       active  running
cbfs_node.3.service                             02d48afd.../10.186.172.24       active  running
couchbase_bootstrap_node.service                02d48afd.../10.186.172.24       active  running
couchbase_bootstrap_node_announce.service       02d48afd.../10.186.172.24       active  running
couchbase_node.1.service                        6ecff20c.../10.51.177.81        active  running
couchbase_node.2.service                        b8eb6653.../10.79.155.153       active  running
</code></p>

<p><strong>View cbfs output</strong></p>

<p><code>
$ fleetctl journal cbfs_node.1.service
2014/11/14 23:18:58 Connecting to couchbase bucket cbfs at http://10.51.177.81:8091/
2014/11/14 23:18:58 Error checking view version: MCResponse status=KEY_ENOENT, opcode=GET, opaque=0, msg: Not found
2014/11/14 23:18:58 Installing new version of views (old version=0)
2014/11/14 23:18:58 Listening to web requests on :8484 as server 10.51.177.81
2014/11/14 23:18:58 Error removing 10.51.177.81's task list: MCResponse status=KEY_ENOENT, opcode=DELETE, opaque=0, msg: Not found
2014/11/14 23:19:05 Error updating space used: Expected 1 result, got []
</code></p>

<h2>Run cbfs client</h2>

<p>Run a bash shell in a docker container that has <code>cbfsclient</code> pre-installed:</p>

<p><code>
$ sudo docker run -ti --net=host tleyden5iwx/cbfs /bin/bash
</code></p>

<p><strong>Upload a file</strong></p>

<p>From within the docker container launched in the previous step:</p>

<p>```</p>

<h1>echo &ldquo;foo&rdquo; > foo</h1>

<h1>ip=$(hostname -i | tr -d &lsquo; &rsquo;)</h1>

<h1>cbfsclient <a href="http://$ip:8484/">http://$ip:8484/</a> upload foo /foo</h1>

<p>```</p>

<p>There should be no errors.  If you run <code>fleetctl journal cbfs_node.1.service</code> again on the CoreOS instance, you should see log messages like:</p>

<p><code>
2014/11/14 21:51:43 Recorded myself as an owner of e242ed3bffccdf271b7fbaf34ed72d089537b42f: result=success
</code></p>

<p><strong>List directory</strong></p>

<p>```</p>

<h1>cbfsclient <a href="http://$ip:8484/">http://$ip:8484/</a> ls /</h1>

<p>foo
```</p>

<p>It should list the foo file we uploaded earlier.</p>

<p>Congratulations!  You now have cbfs up and running.</p>

<h2>References</h2>

<ul>
<li><a href="http://dustin.sallings.org/2012/09/27/cbfs.html">cbfs &ndash; a couchbase large object store</a></li>
<li><a href="http://labs.couchbase.com/cbfs/">cbfs presentation</a></li>
<li><a href="http://cbfs-ext.hq.couchbase.com/dist/">cbfs binary downloads</a></li>
<li><a href="http://github.com/couchbaselabs/cbfs">cbfs github repo</a></li>
<li><a href="https://github.com/couchbaselabs/cbfs/issues/132">cbfs question</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreOS with Nvidia CUDA GPU drivers]]></title>
    <link href="http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers/"/>
    <updated>2014-11-04T07:08:00-08:00</updated>
    <id>http://tleyden.github.io/blog/2014/11/04/coreos-with-nvidia-cuda-gpu-drivers</id>
    <content type="html"><![CDATA[<p>This will walk you through installing the Nvidia GPU kernel module and CUDA drivers on a docker container running inside of CoreOS.</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/coreos-nvidia-gpu.png" alt="architecture diagram" /></p>

<h2>Launch CoreOS on an AWS GPU instance</h2>

<ul>
<li><p>Launch a new EC2 instance</p></li>
<li><p>Under &ldquo;Community AMIs&rdquo;, search for <strong>ami-f669f29e</strong> (CoreOS stable 494.4.0 (HVM))</p></li>
<li><p>Select the GPU instances: <strong>g2.2xlarge</strong></p></li>
<li><p>Increase root EBS store from 8 GB &ndash;> 20 GB to give yourself some breathing room</p></li>
</ul>


<h2>ssh into CoreOS instance</h2>

<p>Find the public ip of the EC2 instance launched above, and ssh into it:</p>

<p><code>
$ ssh -A core@ec2-54-80-24-46.compute-1.amazonaws.com
</code></p>

<h2>Run Ubuntu 14 docker container in privileged mode</h2>

<p><code>
$ sudo docker run --privileged=true -i -t ubuntu:14.04 /bin/bash
</code></p>

<p>After the above command, you should be inside a root shell in your docker container.  The rest of the steps will assume this.</p>

<h2>Install build tools + other required packages</h2>

<p>In order to match the version of gcc that was used to build the CoreOS kernel.  (gcc 4.7)</p>

<p>```</p>

<h1>apt-get update</h1>

<h1>apt-get install gcc-4.7 g++-4.7 wget git make dpkg-dev</h1>

<p>```</p>

<p><strong>Set gcc 4.7 as default</strong></p>

<p>```</p>

<h1>update-alternatives &mdash;remove gcc /usr/bin/gcc-4.8</h1>

<h1>update-alternatives &mdash;install /usr/bin/gcc gcc /usr/bin/gcc-4.7 60 &mdash;slave /usr/bin/g++ g++ /usr/bin/g++-4.7</h1>

<h1>update-alternatives &mdash;install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40 &mdash;slave /usr/bin/g++ g++ /usr/bin/g++-4.8</h1>

<p>```</p>

<p><strong>Verify</strong></p>

<p>```</p>

<h1>update-alternatives &mdash;config gcc</h1>

<p>```</p>

<p>It should list gcc 4.7 with an asterisk next to it:</p>

<p><code>
* 0            /usr/bin/gcc-4.7   60        auto mode
</code></p>

<h2>Prepare CoreOS kernel source</h2>

<p><strong>Clone CoreOS kernel repository</strong></p>

<p><code>
$ mkdir -p /usr/src/kernels
$ cd /usr/src/kernels
$ git clone https://github.com/coreos/linux.git
</code></p>

<p><strong>Find CoreOS kernel version</strong></p>

<p>```</p>

<h1>uname -a</h1>

<p>Linux ip-10-11-167-200.ec2.internal 3.17.2+ #2 SMP Tue Nov 4 04:15:48 UTC 2014 x86_64 Intel&reg; Xeon&reg; CPU E5-2670 0 @ 2.60GHz GenuineIntel GNU/Linux
```</p>

<p>The CoreOS kernel version is <strong>3.17.2</strong></p>

<p><strong>Switch correct branch for this kernel version </strong></p>

<p>```</p>

<h1>cd linux</h1>

<h1>git checkout remotes/origin/coreos/v3.17.2</h1>

<p>```</p>

<p><strong>Create kernel configuration file</strong></p>

<p>```</p>

<h1>zcat /proc/config.gz > /usr/src/kernels/linux/.config</h1>

<p>```</p>

<p><strong>Prepare kernel source for building modules</strong></p>

<p>```</p>

<h1>make modules_prepare</h1>

<p>```</p>

<p>Now you should be ready to install the nvidia driver.</p>

<p><strong>Hack the kernel version</strong></p>

<p>In order to avoid <a href="https://gist.github.com/tleyden/2a46a86056e476976a8e#file-gistfile1-txt-L956">nvidia: version magic errors</a>, the following hack is required:</p>

<p>```</p>

<h1>sed -i -e &rsquo;s/3.17.2/3.17.2+/&lsquo; include/generated/utsrelease.h</h1>

<p>```</p>

<p>I&rsquo;ve <a href="https://groups.google.com/d/msg/coreos-user/CSp_wSywmI4/CBHwocj8v9oJ">posted to the CoreOS Group</a> to ask why this hack is needed.</p>

<h2>Install nvidia driver</h2>

<p><strong>Download</strong></p>

<p>```</p>

<h1>mkdir -p /opt/nvidia</h1>

<h1>cd /opt/nvidia</h1>

<h1>wget <a href="http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run">http://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run</a></h1>

<p>```</p>

<p><strong>Unpack</strong></p>

<p>```</p>

<h1>chmod +x cuda_6.5.14_linux_64.run</h1>

<h1>mkdir nvidia_installers</h1>

<h1>./cuda_6.5.14_linux_64.run -extract=<code>pwd</code>/nvidia_installers</h1>

<p>```</p>

<p><strong>Install</strong></p>

<p>```</p>

<h1>cd nvidia_installers</h1>

<h1>./NVIDIA-Linux-x86_64-340.29.run &mdash;kernel-source-path=/usr/src/kernels/linux/</h1>

<p>```</p>

<p><strong>Installer Questions</strong></p>

<ul>
<li>Install NVidia&rsquo;s 32-bit compatibility libraries? <strong>YES</strong></li>
<li>Would you like to run nvidia-xconfig? <strong>NO</strong></li>
</ul>


<p>If everything worked, you should see:</p>

<p><img src="http://tleyden-misc.s3.amazonaws.com/blog_images/nvidia_driver_installed.png" alt="nvidia drivers installed" /></p>

<p>your <code>/var/log/nvidia-installer.log</code> should look something like <a href="https://gist.github.com/tleyden/6d585fb8ab08154949c8">this</a></p>

<h2>Load nvidia kernel module</h2>

<p>```</p>

<h1>modprobe nvidia</h1>

<p>```</p>

<p>No errors should be returned.  Verify it&rsquo;s loaded by running:</p>

<p>```</p>

<h1>lsmod | grep -i nvidia</h1>

<p>```</p>

<p>and you should see:</p>

<p><code>
nvidia              10533711  0
i2c_core               41189  2 nvidia,i2c_piix4
</code></p>

<h2>Install CUDA</h2>

<p>In order to fully verify that the kernel module is working correctly, install the CUDA drivers + library and run a device query.</p>

<p>To install CUDA:</p>

<p>```</p>

<h1>./cuda-linux64-rel-6.5.14-18749181.run</h1>

<h1>./cuda-samples-linux-6.5.14-18745345.run</h1>

<p>```</p>

<h2>Verify CUDA</h2>

<p>```</p>

<h1>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</h1>

<h1>make</h1>

<h1>./deviceQuery</h1>

<p>```</p>

<p>You should see the following output:</p>

<p><code>
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 6.5, CUDA Runtime Version = 6.5, NumDevs = 1, Device0 = GRID K520
Result = PASS
</code></p>

<p>Congratulations!  You now have a docker container running under CoreOS that can access the GPU.</p>

<h1>Appendix: Expose GPU to other docker containers</h1>

<p>If you need <em>other</em> docker containers on this CoreOS instance to be able to access the GPU, you can do the following steps.</p>

<p><strong>Exit docker container</strong></p>

<p>```</p>

<h1>exit</h1>

<p>```</p>

<p>You should be back to your CoreOS shell.</p>

<p><strong>Add nvidia device nodes</strong></p>

<p><code>
$ wget https://gist.githubusercontent.com/tleyden/74f593a0beea300de08c/raw/95ed93c5751a989e58153db6f88c35515b7af120/nvidia_devices.sh
$ chmod +x nvidia_devices.sh
$ sudo ./nvidia_devices.sh
</code></p>

<p><strong>Verify device nodes</strong></p>

<p><code>
$ ls -alh /dev | grep -i nvidia
crw-rw-rw-  1 root root  251,   0 Nov  5 16:37 nvidia-uvm
crw-rw-rw-  1 root root  195,   0 Nov  5 16:37 nvidia0
crw-rw-rw-  1 root root  195, 255 Nov  5 16:37 nvidiactl
</code></p>

<p><strong>Launch docker containers</strong></p>

<p>When you launch other docker containers on the same CoreOS instance, to allow them to access the GPU device you will need to add the following arguments:</p>

<p><code>
$ sudo docker run -ti --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm tleyden5iwx/ubuntu-cuda /bin/bash
</code></p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/tleyden/tleyden.github.io/blob/6d71759cc5e530efcae10d9c6012dd217f76795c/source/_posts/2014-11-04-coreos-with-nvidia-cuda-gpu-drivers.markdown">Previous version of this blog post</a></li>
<li><a href="https://groups.google.com/forum/#!topic/coreos-user/CSp_wSywmI4">CoreOS Google Group thread</a> &ndash; Thanks Сергей!</li>
<li><a href="http://tleyden.github.io/blog/2014/10/25/docker-on-aws-gpu-ubuntu-14-dot-04-slash-cuda-6-dot-5/">Docker on AWS GPU Ubuntu 14.04 / CUDA 6.5</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
